Analysis tying results to cache locality, alignment, vector width, arithmetic intensity, and roofline predictions.:
The results demonstrate that SIMD speedup is heavily constrained by memory bandwidth rather than computational capability. As array sizes exceed L1/L2 cache capacities, performance becomes memory-bound, limiting SIMD gains. The little to no speedups observed, typically 1.005-0.995x, quickly drop to as low as .97. This aligns with roofline predictions given the low arithmetic intensity of the kernels, like SAXPY at only about 0.17 FLOPs/byte. Alignment effects were minimal due to modern CPU handling of unaligned accesses, while vector width advantages were largely negated by memory bandwidth saturation. The data confirms that for memory-bound workloads in particular, SIMD provides limited benefit regardless of vector lane count or alignment optimization.

Limitations/anomalies with hypotheses (e.g., denormals, thermal effects, frequency scaling, page faults, TLB behavior).:
I'm not entirely sure why there was only a speedup of approximately 1. By disassembling the binaries it becomes obvious that the programs are being compiled correctly, with vectorization when they should and without when they shouldn't. The measurement methodology itself introduced overhead through timing calls and data re-initialization between runs. Additionally, TLB misses and page faults at larger working set sizes created performance variations and added noise. I locked my CPU to only 2.3GHz so it couldn't boost. I have a good cooling setup for a laptop and tried both giving time between tests as well as running the tests constantly back to back to keep it warm. My best guess is that the compiler's conservative auto-vectorization approach, avoiding more aggressive optimizations that could yield greater speedups, further constrained observable performance improvements.
